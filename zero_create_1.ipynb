{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 交差エントロピー"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "普通の交差エントロピーの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\boldsymbol{y}$は，ワンホットエンコーディングされた，ベクトルである，\\\n",
    "よってyの総和は1になっている．\n",
    "\n",
    "一つ目の例では．データ1個に対して損失を計算している．\\\n",
    "二つ目では，データ2個に対して損失を計算している．\n",
    "\n",
    "各レコードの総和は1になっている．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    print(\"元のデータサイズ\", y.shape)\n",
    "    print(\"元のデータサイズ\", t.shape)\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "    print(\"reshape後のデータサイズ\", y.shape)\n",
    "    print(\"reshape後のデータサイズ\", t.shape)\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t * np.log(y + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "元のデータサイズ (3,)\n",
      "元のデータサイズ (3,)\n",
      "reshape後のデータサイズ (1, 3)\n",
      "reshape後のデータサイズ (1, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.3566748010815999)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy_error(np.array([0.1, 0.2, 0.7]), np.array([0, 0, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "元のデータサイズ (2, 3)\n",
      "元のデータサイズ (2, 3)\n",
      "reshape後のデータサイズ (2, 3)\n",
      "reshape後のデータサイズ (2, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.2899091136979087)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 正解データが3次元配列の場合．\n",
    "cross_entropy_error(np.array([[0.1, 0.2, 0.7], [0.8, 0.1, 0.1]]), np.array([[0, 0, 1], [1, 0, 0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ワンホットエンコーディングでないバージョン"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    print(\"元のデータサイズ\", y.shape)\n",
    "    print(\"元のデータサイズ\", t.shape)\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "    print(\"reshape後のデータサイズ\", y.shape)\n",
    "    print(\"reshape後のデータサイズ\", t.shape)\n",
    "    batch_size = y.shape[0]\n",
    "\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0.7\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.2\u001b[39m])\n\u001b[1;32m      2\u001b[0m t \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m----> 3\u001b[0m \u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    }
   ],
   "source": [
    "y = np.array([0.7, 0.1, 0.2])\n",
    "t = np.array([2, 7, 0])\n",
    "y[np.arange(y.shape[0]), t]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "なんかうまくいかないわ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "元のデータサイズ (3,)\n",
      "元のデータサイズ (3,)\n",
      "reshape後のデータサイズ (1, 3)\n",
      "reshape後のデータサイズ (1, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(6.214605598423317)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy_error(np.array([0.1, 0.2, 0.7]), np.array([0, 0, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fとf()の違い．\n",
    "\n",
    "以下のようなコードがある．\n",
    "\n",
    "関数であればf()で渡すべきなのに，fだけで内部の変数が渡せているのは何故だろうか．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "\n",
    "        return loss\n",
    "\n",
    "x = np.array([0.6, 0.9])\n",
    "t = np.array([0, 0, 1])\n",
    "\n",
    "net = simpleNet()\n",
    "\n",
    "f = lambda w: net.loss(x, t)\n",
    "dW = numerical_gradient(f, net.W)\n",
    "\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これは、Pythonにおける関数オブジェクトの重要な性質に関係しています。\n",
    "\n",
    "### ステップバイステップの説明\n",
    "\n",
    "1. まず、コードの該当部分を確認します："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda w: net.loss(x, t)\n",
    "dW = numerical_gradient(f, net.W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "2. この場合：\n",
    "- `f` はラムダ関数で定義された関数オブジェクト\n",
    "- `f` は変数名であり、関数そのものを参照\n",
    "- `f()` は関数の実行を意味する\n",
    "\n",
    "3. `numerical_gradient` 関数は以下のような動作をします：\n",
    "- 第1引数として関数オブジェクト自体を受け取る\n",
    "- 内部で必要に応じてその関数を実行する\n",
    "\n",
    "### 重要なポイント\n",
    "\n",
    "- `f` と `f()` の違い：\n",
    "  - `f`: 関数オブジェクトそのもの（参照）\n",
    "  - `f()`: 関数の実行\n",
    "\n",
    "- Pythonでは関数は第一級オブジェクトとして扱われます：\n",
    "  - 変数に代入可能\n",
    "  - 他の関数の引数として渡すことが可能\n",
    "  - 関数から返すことが可能\n",
    "\n",
    "### 具体例\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerical_gradientの簡略化された内部実装例\n",
    "def numerical_gradient(func, x):\n",
    "    # funcは関数オブジェクト\n",
    "    # 内部で必要に応じてfunc(x)として実行する\n",
    "    result = func(x)  # ここで実際に関数を呼び出し\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "したがって、`numerical_gradient(f, net.W)`では、関数オブジェクト`f`自体を渡し、`numerical_gradient`関数の中で必要に応じて実行されるという仕組みになっています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 要点\n",
    "\n",
    "fは関数のオブジェクト\n",
    "\n",
    "fって変数に関数そのものが入っている．．実行するのは，他の関数の内部．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# りんごのやつの逆伝播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "りんごの値段を$t$とする．んで，なんかりんごの値段は220円になったらしい．\\\n",
    "→この状況の時，$t$の微小な変化に対して，最終的な値段がどれだけ変化するかを求める．\n",
    "\n",
    "という問題．\n",
    "\n",
    "んで，中身では，以下のような処理が行われている．(順伝播)\n",
    "$$\n",
    "\\begin{align*}\n",
    "x_1 &= 2t\\\\\n",
    "x_2 &= 1.1x_1\\\\\n",
    "\\text{total} &= x_2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "これを逆伝播することによって答え($t$の微小な変化に対する，最終的な値段の変化)が求められる．\\\n",
    "実際にやってみる．\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{d\\text{total}}{dt} &= \\frac{d\\text{total}}{dx_2} \\cdot \\frac{dx_2}{dx_1} \\cdot \\frac{dx_1}{dt}\\\\\n",
    "\\\\\n",
    "\\frac{d\\text{total}}{dx_2} &= 1\\\\\n",
    "\\frac{dx_2}{dx_1} &= 1.1\\\\\n",
    "\\frac{dx_1}{dt} &= 2\\\\\n",
    "\\\\\n",
    "\\frac{d\\text{total}}{dt} &= 1 \\cdot 1.1 \\cdot 2 = 2.2\n",
    "\\end{align*}\n",
    "$$\n",
    "逆伝播によって求められた．\n",
    "\n",
    "これのなにがすごいの？と思うかもしれない．\n",
    "\n",
    "問題をもう一度考えてみよう．\n",
    "\n",
    "りんごの値段を$t$とする(入力)．んで，なんかりんごの値段は220円になった(出力)\\\n",
    "今，入力と出力が与えられているね，これをニューラルネットの入力と出力と考える．\n",
    "\n",
    "これだけの情報では，うまくパラメータを更新することができない．\\\n",
    "(本当は中身で複雑な処理が行われているけど，入力と出力のみからでは，いい感じのパラメータをみつけるための勾配が算出できない．)\n",
    "\n",
    "しかし，途中の計算結果．$x_1，x_2$を残しておくことで，出力から逆向きに計算して，勾配を求めることができる．ということ．\n",
    "\n",
    "本当は複雑なんだけど，途中の単純な計算を残しておくことで，複雑なニューラルネットのパラメータの更新に使うことができる．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0]\n",
      " [0 4]]\n",
      "[[1 0]\n",
      " [0 4]]\n"
     ]
    }
   ],
   "source": [
    "ReLu = Relu()\n",
    "out = ReLu.forward(np.array([[1, -2], [-3, 4]]))\n",
    "print(out)\n",
    "dx = ReLu.backward(out)\n",
    "print(dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# np.dotの挙動の確認"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "入力：$X$，重み：$W$，バイアス：$B$，出力：$Y$とする．\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "X &= \\begin{bmatrix} X_1 \\\\ X_2 \\end{bmatrix}\\\\\n",
    "W &= \\begin{bmatrix} W_{11} & W_{12} & W_{13} \\\\ W_{21} & W_{22} & W_{23} \\end{bmatrix}\\\\\n",
    "B &= \\begin{bmatrix} B_1 \\\\ B_2 \\\\ B_3 \\end{bmatrix}\\\\\n",
    "Y &= \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ Y_3 \\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input (2,)\n",
      "[0.5488135  0.71518937]\n",
      "----------\n",
      "weght (2, 3)\n",
      "[[0.60276338 0.54488318 0.4236548 ]\n",
      " [0.64589411 0.43758721 0.891773  ]]\n",
      "----------\n",
      "bias (3,)\n",
      "[0.96366276 0.38344152 0.79172504]\n"
     ]
    }
   ],
   "source": [
    "# randomのseedを固定\n",
    "np.random.seed(0)\n",
    "X = np.random.rand(2)\n",
    "W = np.random.rand(2, 3)\n",
    "B = np.random.rand(3)\n",
    "print(\"input\", X.shape)\n",
    "print(X)\n",
    "print(\"-\" * 10)\n",
    "print(\"weght\", W.shape)\n",
    "print(W)\n",
    "print(\"-\" * 10)\n",
    "print(\"bias\", B.shape)\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output (3,)\n",
      "[1.75640404 0.99543849 1.66201908]\n"
     ]
    }
   ],
   "source": [
    "Y = np.dot(X, W) + B\n",
    "print(\"output\", Y.shape)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "違和感がある．\n",
    "\n",
    "$X$は2×1行列，$W$は2×3の行列と捉えられるのに，エラーが出ずに計算できるのはおかしい．\n",
    "\n",
    "numpy.dot()の挙動を見てみる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今回のやつ\n",
      "[0.79274128 0.61199697 0.87029404]\n",
      "----------\n",
      "計算として，正しくなるように転置した方\n",
      "[0.79274128 0.61199697 0.87029404]\n"
     ]
    }
   ],
   "source": [
    "print(\"今回のやつ\")\n",
    "print(np.dot(X, W))\n",
    "print(\"-\" * 10)\n",
    "print(\"計算として，正しくなるように転置した方\")\n",
    "print(np.dot(X.T, W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "結果は同じになっている．\n",
    "\n",
    "numpy.dot()はここをどう扱っているんだろう．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "明示的にXの形状を，1×2行列にしてみる．\n",
    "$$\n",
    "X = \\begin{bmatrix} X_1 & X_2 \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1×2 @ 2×3\n",
      "[[0.79274128 0.61199697 0.87029404]]\n",
      "----------\n",
      "2×1 @ 2×3\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (2,1) and (2,3) not aligned: 1 (dim 1) != 2 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2×1 @ 2×3\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (2,1) and (2,3) not aligned: 1 (dim 1) != 2 (dim 0)"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "X = np.random.rand(1, 2)\n",
    "print(\"1×2 @ 2×3\")\n",
    "print(np.dot(X, W))\n",
    "print(\"-\" * 10)\n",
    "print(\"2×1 @ 2×3\")\n",
    "print(np.dot(X.T, W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "明示的にXの形状を，2×1行列にしてみる．\n",
    "$$\n",
    "X = \\begin{bmatrix} X_1 \\\\ X_2 \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1×2 @ 2×3\n",
      "[[0.79274128 0.61199697 0.87029404]]\n",
      "----------\n",
      "2×1 @ 2×3\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (2,1) and (2,3) not aligned: 1 (dim 1) != 2 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2×1 @ 2×3\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (2,1) and (2,3) not aligned: 1 (dim 1) != 2 (dim 0)"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "X = np.random.rand(2, 1)\n",
    "print(\"1×2 @ 2×3\")\n",
    "print(np.dot(X.T, W))\n",
    "print(\"-\" * 10)\n",
    "print(\"2×1 @ 2×3\")\n",
    "print(np.dot(X, W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "明示的に形状を指定するとエラーが出た．\n",
    "\n",
    "np.random.rand(n)で生成する一次元配列は，n個の要素を持つ配列として扱われる．\n",
    "だから都合の良い形状としてとられるぽいので，あんま気にしなくて良さそう．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 内積の逆伝播\n",
    "\n",
    "参考：[Affineレイヤの逆伝播を地道に成分計算する](https://qiita.com/yuyasat/items/d9cdd4401221df5375b6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "入力：$X$，重み：$W$，バイアス：$B$，出力：$Y$とする．\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "X &= \\begin{bmatrix} X_1 & X_2 \\end{bmatrix}\\\\\n",
    "W &= \\begin{bmatrix} W_{11} & W_{12} & W_{13} \\\\ W_{21} & W_{22} & W_{23} \\end{bmatrix}\\\\\n",
    "B &= \\begin{bmatrix} B_1 & B_2 & B_3 \\end{bmatrix}\\\\\n",
    "Y &= \\begin{bmatrix} Y_1 & Y_2 & Y_3 \\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Xはただの一次元配列だからこうする意味もあまりないけど．違和感があるので明示的に行列の形状を揃えておく．\n",
    "\n",
    "損失関数を$L$とおく．\n",
    "\n",
    "今回内積の逆伝播がどうなるかを見てみたい．\n",
    "\n",
    "逆伝播の流れは以下の通り．\n",
    "- 損失関数$L$を$Y$で微分\n",
    "- $B$は，$Y$に対して加算されるだけなので，$B$に関しては，$L$を$Y$で微分した値がそのまま逆伝播する．\n",
    "ここまでを数式で表すとこんな感じ．\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial Y} &= \\begin{bmatrix} \\frac{\\partial L}{\\partial Y_1} & \\frac{\\partial L}{\\partial Y_2} & \\frac{\\partial L}{\\partial Y_3} \\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$\n",
    "これだけ．これは次の材料になる．\n",
    "\n",
    "ここから$X$の逆伝播，$W$の逆伝播を求める．\n",
    "\n",
    "- $X$の逆伝播\n",
    "損失関数に対しての$X$の偏微分．$L$を$X$の各要素について微分した形になっている．\\\n",
    "$X$の形状は，1×2行列なので，その形状を維持する．\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial X} &= \\begin{bmatrix}\\frac{\\partial L}{\\partial x_1} & \\frac{\\partial L}{\\partial x_2}\\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$\n",
    "$\\frac{\\partial L}{\\partial x_1}$は直接計算できない．なぜなら間に$Y$を挟んでいるから．\\\n",
    "連鎖律を使って計算する．\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial Y}\\frac{\\partial Y}{\\partial X} &=\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial L}{\\partial Y}\\frac{\\partial Y}{\\partial x_1} & \\frac{\\partial L}{\\partial Y}\\frac{\\partial Y}{\\partial x_2}\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$\n",
    "$Y$が$X$を使って直接計算してるので，$\\frac{\\partial Y}{\\partial x_1}$は計算できる．\\\n",
    "しかも，$\\frac{\\partial L}{\\partial Y}$はすでに計算済みなので，これを使える．\n",
    "$$\n",
    "Y=XWより，\\\\\n",
    "\\begin{align*}\n",
    "\\frac{\\partial Y}{\\partial x_1} &=\n",
    "\\begin{bmatrix} \\frac{\\partial{Y_1}}{\\partial{x_1}} & \\frac{\\partial{Y_2}}{\\partial{x_1}} & \\frac{\\partial{Y_3}}{\\partial{x_1}} \\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$\n",
    "ちなみに，$x_1$はスカラーなので，偏微分した後のベクトルは列でも行でもどっちでもいい．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 偏微分の話\n",
    "$a$(スカラー)を$\\boldsymbol{b}$(ベクトル)で微分した結果は，$\\boldsymbol{b}$の形状に依存する．\n",
    "$$\n",
    "\\boldsymbol{a}\\in \\mathbb{R}\\\\\n",
    "\\boldsymbol{b}\\in \\mathbb{R}^m\n",
    "\n",
    "\\\\\n",
    "\\frac{\\partial{a}}{\\partial{\\boldsymbol{b}}} =\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial{a}}{\\partial{b_1}} & \\frac{\\partial{a}}{\\partial{b_2}} & \\cdots & \\frac{\\partial{a}}{\\partial{b_m}}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$\\boldsymbol{a}$(ベクトル)を$b$(スカラー)で微分した結果は，aの形状に依存する．\n",
    "$$\n",
    "\\boldsymbol{a}\\in \\mathbb{R}^n\\\\\n",
    "\\boldsymbol{b}\\in \\mathbb{R}\n",
    "\n",
    "\\\\\n",
    "\\frac{\\partial{\\boldsymbol{a}}}{\\partial{b}} =\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial{a_1}}{\\partial{b}} & \\frac{\\partial{a_2}}{\\partial{b}} & \\cdots & \\frac{\\partial{a_n}}{\\partial{b}}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$\\boldsymbol{a}$(ベクトル)を$\\boldsymbol{b}$(ベクトル)で微分した結果は，ヤコビ行列になる．\n",
    "$$\n",
    "\n",
    "\\boldsymbol{a}\\in \\mathbb{R}^n\\\\\n",
    "\\boldsymbol{b}\\in \\mathbb{R}^m\n",
    "\n",
    "\\\\\n",
    "\n",
    "\\mathbf{J} = \\frac{\\partial \\mathbf{a}}{\\partial \\mathbf{b}} =\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial a_1}{\\partial b_1} & \\frac{\\partial a_1}{\\partial b_2} & \\cdots & \\frac{\\partial a_1}{\\partial b_m} \\\\\n",
    "\\frac{\\partial a_2}{\\partial b_1} & \\frac{\\partial a_2}{\\partial b_2} & \\cdots & \\frac{\\partial a_2}{\\partial b_m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial a_n}{\\partial b_1} & \\frac{\\partial a_n}{\\partial b_2} & \\cdots & \\frac{\\partial a_n}{\\partial b_m}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
